{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                       _oo0oo_\n",
    "#                      o8888888o\n",
    "#                      88\" . \"88\n",
    "#                      (| -_- |)\n",
    "#                      0\\  =  /0\n",
    "#                    ___/`---'\\___\n",
    "#                  .' \\\\|     |# '.\n",
    "#                 / \\\\|||  :  |||# \\\n",
    "#                / _||||| -:- |||||- \\\n",
    "#               |   | \\\\\\  -  #/ |   |\n",
    "#               | \\_|  ''\\---/''  |_/ |\n",
    "#               \\  .-\\__  '-'  ___/-. /\n",
    "#             ___'. .'  /--.--\\  `. .'___\n",
    "#          .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
    "#         | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
    "#         \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
    "#     =====`-.____`.___ \\_____/___.-`___.-'=====\n",
    "#                       `=---='\n",
    "#\n",
    "#\n",
    "#     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "#               佛祖保佑         永无BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '1'\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = '1'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "import implicit\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.evaluation import train_test_split, ndcg_at_k, precision_at_k\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import BallTree\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from joblib import Parallel as JoblibParallel, delayed\n",
    "from sklearn.metrics import auc, roc_auc_score\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from functools import partial\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import isspmatrix_csr\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_millis_time():\n",
    "    return int(time.time() * 1000)\n",
    "\n",
    "\n",
    "def format_time(interval):\n",
    "    interval = relativedelta(microseconds=(interval * 10 ** 6))\n",
    "    amounts = interval.days, interval.hours, interval.minutes, interval.seconds, interval.microseconds / 10 ** 3\n",
    "    result = ' '.join(\n",
    "        u'{} {}'.format(int(amount), label)\n",
    "        for amount, label in zip(amounts, ['d', 'h', 'm', 's', 'ms']) if amount\n",
    "    )\n",
    "    if not result:\n",
    "        result = u'0 ms'\n",
    "    return result\n",
    "\n",
    "\n",
    "class TimeLatch(object):\n",
    "    def __init__(self):\n",
    "        self._start_time = time.time()\n",
    "\n",
    "    def took(self):\n",
    "        start_time = self._start_time\n",
    "        self._start_time = time.time()  # finish time\n",
    "        return self._start_time - start_time\n",
    "\n",
    "    def current(self):\n",
    "        return time.time() - self._start_time\n",
    "\n",
    "    def took_msg(self):\n",
    "        return format_time(self.took())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(array):\n",
    "    if sp.issparse(array):\n",
    "        return np.asarray(np.sqrt(array.power(2).sum(axis=1)))\n",
    "    else:\n",
    "        return np.linalg.norm(array, axis=1, keepdims=True)\n",
    "\n",
    "def dot(left, right):\n",
    "    if isspmatrix_csr(left):\n",
    "        return np.asarray(left.multiply(right).sum(axis=1))\n",
    "    return np.multiply(left, right).sum(axis=1, keepdims=True)\n",
    "\n",
    "def cosine(left, right):\n",
    "    result = dot(left, right) / (_norm(left) * _norm(right))\n",
    "    return np.maximum(1. - result, 0.)\n",
    "\n",
    "def braycurtis(left, right):\n",
    "    abs_diff = np.abs(left - right)\n",
    "    abs_sum = np.abs(left + right)\n",
    "    if sp.issparse(left):\n",
    "        return 1. * np.asarray(abs_diff.sum(axis=1)) / np.asarray(abs_sum.sum(axis=1))\n",
    "    else:\n",
    "        return 1. * abs_diff.sum(axis=1, keepdims=True) / abs_sum.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel(JoblibParallel):\n",
    "    def __call__(self, iterable):\n",
    "        if self.n_jobs == 1:\n",
    "            return [function(*args, **kwargs) for function, args, kwargs in iterable]\n",
    "        else:\n",
    "            return super(Parallel, self).__call__(iterable)\n",
    "\n",
    "def partition_indices(total, part, nparts):\n",
    "    if not (0 <= part < nparts):\n",
    "        raise ValueError()\n",
    "    part_size = total // nparts\n",
    "    left = total % nparts\n",
    "    lower = part_size * part + min(left, part)\n",
    "    return lower, lower + part_size + (1 if part < left else 0)        \n",
    "        \n",
    "def partition(array, nparts):\n",
    "    return [\n",
    "        array[left_index:right_index]\n",
    "        for left_index, right_index in (partition_indices(len(array), part, nparts) for part in range(nparts))\n",
    "    ]\n",
    "\n",
    "def _group(group_sizes, *arrays):\n",
    "    left_index = 0\n",
    "    for group_size in group_sizes:\n",
    "        yield [array[left_index:left_index + group_size] for array in arrays]\n",
    "        left_index += group_size\n",
    "        \n",
    "def get_serp_sizes(serp_values):\n",
    "    serp_values = np.array(serp_values)\n",
    "    serp_values = serp_values.flatten()\n",
    "    _, serp_value_idx = np.unique(serp_values, return_index=True)\n",
    "    return np.diff(np.sort(np.concatenate([serp_value_idx, np.array([len(serp_values)])])))\n",
    "\n",
    "def _compute_single_ndcg(y_true, y_pred, cut_off=10):\n",
    "    if cut_off is None:\n",
    "        cut_off = len(y_true)\n",
    "\n",
    "    if np.unique(y_true).size == 1:\n",
    "        return np.nan\n",
    "\n",
    "    actual = (y_true[np.argsort(-y_pred)])[:cut_off]\n",
    "    ideal = np.sort(y_true)[::-1][:cut_off]\n",
    "\n",
    "    size = actual.shape[0]\n",
    "    denominator = np.log2(np.arange(size) + 2)\n",
    "    dcg = ((2 ** actual - 1) / denominator).sum()\n",
    "    idcg = ((2 ** ideal - 1) / denominator).sum()\n",
    "    return dcg / idcg\n",
    "\n",
    "def _compute_ndcg(y_true, y_pred, groups, cut_off=10, mean=True):\n",
    "    ndcg = np.array([\n",
    "        _compute_single_ndcg(group_target, group_pred, cut_off)\n",
    "        for group_target, group_pred in _group(groups, y_true, y_pred)\n",
    "    ])\n",
    "    return np.nanmean(ndcg) if mean else ndcg\n",
    "\n",
    "def _compute_single_mauc(y_true, y_pred):\n",
    "    if np.unique(y_true).size == 1:\n",
    "        return np.nan\n",
    "\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def _compute_mauc(y_true, y_pred, groups, _, mean=True):\n",
    "    mauc = np.array([\n",
    "        _compute_single_mauc(group_target, group_pred)\n",
    "        for group_target, group_pred in _group(groups, y_true, y_pred)\n",
    "    ])\n",
    "    return np.nanmean(mauc) if mean else mauc\n",
    "\n",
    "def _compute_ap(y_true, y_pred, cut_off=10):\n",
    "    order = np.argsort(-y_pred)\n",
    "    truth = (y_true[order] > 0.0).astype(float)\n",
    "\n",
    "    positive = sum(truth)\n",
    "    if positive == 0.0 or positive == len(truth):\n",
    "        return np.nan\n",
    "\n",
    "    cut_off_ = cut_off if cut_off is not None else len(truth)\n",
    "\n",
    "    denominator = min(cut_off_, positive)\n",
    "    truth.resize(cut_off_)\n",
    "    return np.divide(np.multiply(truth, np.cumsum(truth)), 1.0 + np.arange(cut_off_)).sum() / denominator\n",
    "\n",
    "def _compute_map(y_true, y_pred, groups, cut_off=10, mean=True):\n",
    "    ap = np.array([\n",
    "        _compute_ap(group_target, group_pred, cut_off)\n",
    "        for group_target, group_pred in _group(groups, y_true, y_pred)\n",
    "    ])\n",
    "    return np.nanmean(ap) if mean else ap\n",
    "\n",
    "def _compute_single_recall(y_true, y_pred, cut_off=10):\n",
    "    order = np.argsort(-y_pred)\n",
    "    truth = (y_true[order] > 0.0).astype(float)\n",
    "\n",
    "    positive = sum(truth)\n",
    "    if positive == 0.0 or positive == len(truth):\n",
    "        return np.nan\n",
    "\n",
    "    cut_off_ = cut_off if cut_off is not None else len(truth)\n",
    "\n",
    "    denominator = min(cut_off_, positive)\n",
    "    truth.resize(cut_off_)\n",
    "    return 1. * truth[:cut_off_].sum() / denominator\n",
    "\n",
    "def _compute_recall(y_true, y_pred, groups, cut_off=10, mean=True):\n",
    "    ap = np.array([\n",
    "        _compute_single_recall(group_target, group_pred, cut_off)\n",
    "        for group_target, group_pred in _group(groups, y_true, y_pred)\n",
    "    ])\n",
    "    return np.nanmean(ap) if mean else ap    \n",
    "\n",
    "\n",
    "def _prepare_diversity_entities_group(entity, y_pred, cut_off=10):\n",
    "    order = np.argsort(-y_pred)\n",
    "    cut_off_ = cut_off if cut_off is not None else len(entity)\n",
    "    return entity[order][:cut_off]\n",
    "\n",
    "def _prepare_diversity_entities(entity, y_pred, groups, cut_off=10, mean=True):\n",
    "    result = [\n",
    "        _prepare_diversity_entities_group(group_target, group_pred, cut_off)\n",
    "        for group_target, group_pred in _group(groups, entity, y_pred)\n",
    "    ]\n",
    "    return np.hstack(result)  \n",
    "    \n",
    "def _compute_ranking_metric(metric_computer, y_true, y_pred, groups, cut_off, mean, ncores):\n",
    "    groups_parts = partition(groups, ncores)\n",
    "    metrics = Parallel(n_jobs=ncores)(\n",
    "        delayed(metric_computer)(y_true_part, y_pred_part, groups_part, cut_off, mean=False)\n",
    "        for (y_true_part, y_pred_part), groups_part in zip(_group(map(sum, groups_parts), y_true, y_pred), groups_parts)\n",
    "    )\n",
    "    metrics = np.hstack(metrics)\n",
    "    return np.nanmean(metrics) if mean else metrics\n",
    "\n",
    "\n",
    "def compute_map(y_true, y_pred, groups, ncores, cut_off=10, mean=True):\n",
    "    return _compute_ranking_metric(_compute_map, y_true, y_pred, groups, cut_off, mean, ncores)\n",
    "\n",
    "\n",
    "def compute_ndcg(y_true, y_pred, groups, ncores, cut_off=10, mean=True):\n",
    "    return _compute_ranking_metric(_compute_ndcg, y_true, y_pred, groups, cut_off, mean, ncores)\n",
    "\n",
    "\n",
    "def compute_mauc(y_true, y_pred, groups, ncores, cut_off=None, mean=True):\n",
    "    return _compute_ranking_metric(_compute_mauc, y_true, y_pred, groups, None, mean, ncores)\n",
    "\n",
    "\n",
    "def compute_recall(y_true, y_pred, groups, ncores, cut_off=10, mean=True):\n",
    "    return _compute_ranking_metric(_compute_recall, y_true, y_pred, groups, cut_off, mean, ncores)\n",
    "\n",
    "\n",
    "def compute_entropy_diversity(y_true, y_pred, groups, ncores, cut_off=10, mean=True):\n",
    "    entities = _compute_ranking_metric(_prepare_diversity_entities, y_true, y_pred, groups, cut_off, False, ncores)\n",
    "    counts = pd.DataFrame(entities).value_counts().values\n",
    "    counts = counts / len(entities)\n",
    "    return (-counts * np.log(counts)).sum()\n",
    "\n",
    "\n",
    "def compute_coverage(y_true, y_pred, groups, ncores, cut_off=10, mean=True):\n",
    "    entities = _compute_ranking_metric(_prepare_diversity_entities, y_true, y_pred, groups, cut_off, False, ncores)\n",
    "    return len(set(entities))\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self, name, target='target', pred='pred', group='customer_id', ncores=1):\n",
    "        self.name = name\n",
    "        self.target = target\n",
    "        self.pred = pred\n",
    "        self.group = group\n",
    "        self.ncores = ncores\n",
    "\n",
    "    def compute_score(self, target, prediction, ncores, group=None):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        pred = df[self.pred]\n",
    "        target = df[self.target]\n",
    "        group = df.groupby(self.group).target.count().values\n",
    "        return self.compute_score(target, pred, self.ncores, group)\n",
    "        \n",
    "class RankingCutoffMetric(Metric):\n",
    "    def __init__(self, name, computer, cut_off=None):\n",
    "        if cut_off is not None:\n",
    "            name = '{}@{}'.format(name, cut_off)\n",
    "        super().__init__(name)\n",
    "        self.cut_off = cut_off\n",
    "        self.computer = computer\n",
    "\n",
    "    def compute_score(self, target, prediction, ncores, group=None):\n",
    "        if group is None:\n",
    "            raise ValueError('group must be provided for {}'.format(self.name))\n",
    "        target = np.array(target)\n",
    "        prediction = np.array(prediction)\n",
    "        return self.computer(target, prediction, group, ncores, self.cut_off)\n",
    "    \n",
    "\n",
    "class MapMetric(RankingCutoffMetric):\n",
    "    def __init__(self, cut_off=None):\n",
    "        super().__init__('map', compute_map, cut_off)\n",
    "\n",
    "\n",
    "class NdcgMetric(RankingCutoffMetric):\n",
    "    def __init__(self, cut_off=None):\n",
    "        super().__init__('ndcg', compute_ndcg, cut_off)\n",
    "\n",
    "\n",
    "class MeanAucMetric(RankingCutoffMetric):\n",
    "    def __init__(self):\n",
    "        super().__init__('mauc', compute_mauc)\n",
    "        \n",
    "class RecallMetric(RankingCutoffMetric):\n",
    "    def __init__(self, cut_off=None):\n",
    "        super().__init__('recall', compute_recall, cut_off)\n",
    "        \n",
    "class EntropyDiversityMetric(RankingCutoffMetric):\n",
    "    def __init__(self, entity, cut_off=None):\n",
    "        super().__init__(f'entropy_diversity_of_{entity}', compute_entropy_diversity, cut_off)\n",
    "        self.ncores = 36\n",
    "        self.target = entity\n",
    "        \n",
    "class CoverageMetric(RankingCutoffMetric):\n",
    "    def __init__(self, entity, cut_off=None):\n",
    "        super().__init__(f'coverage_of_{entity}', compute_coverage, cut_off)\n",
    "        self.ncores = 36\n",
    "        self.target = entity\n",
    "        \n",
    "METRICS = [MeanAucMetric()]\n",
    "for k in (1, 5, 15):\n",
    "    METRICS += [NdcgMetric(k), RecallMetric(k),\n",
    "                EntropyDiversityMetric('chain_id', k), EntropyDiversityMetric('default_product_group_id', k),\n",
    "                CoverageMetric('chain_id', k), CoverageMetric('default_product_group_id', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_made_restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# remove customer_id = -10 - as unknown customer\n",
    "df = df[df['customer_id'] != -10].reset_index(drop=True)\n",
    "\n",
    "# remove product_group_ids is None - not a restaurants, they are food markets\n",
    "df = df[~df['product_group_ids'].isnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove fastfood from validation\n",
    "fastfood = {\n",
    "    'Макдоналдс': df[df['chain_name'] == 'Макдоналдс']['chain_id'].unique().tolist(),\n",
    "    'KFC': df[df['chain_name'] == 'KFC']['chain_id'].unique().tolist(),\n",
    "    'Burger King': df[df['chain_name'] == 'Burger King']['chain_id'].unique().tolist(),\n",
    "}\n",
    "\n",
    "fastfoold_chains = np.hstack([chain_id for chain_id in fastfood.values()])\n",
    "\n",
    "df = df[~df['chain_id'].isin(fastfoold_chains)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['week'] = (df['date'] - df['date'].min()).dt.days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete customers with only one order\n",
    "customer_ids = df.groupby('customer_id')['chain_id'].nunique()\n",
    "customer_ids = customer_ids[customer_ids > 1].index.tolist()\n",
    "df = df[df['customer_id'].isin(customer_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute base features\n",
    "df.loc[:, 'discount_perc'] = (df['discount_value'] / (df['total_value'] - df['delivery_fee'])).values\n",
    "df.loc[:, 'total_paid_value'] = (df['total_value']  - df['delivery_fee'] - df['discount_value']).values\n",
    "df.loc[:, 'total_food_value'] = (df['total_value'] - df['delivery_fee']).values\n",
    "\n",
    "df.loc[:, 'discount_perc'] = df['discount_perc'].replace(np.inf, 0).clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data on order, customer and vendor entities\n",
    "pairs = df[['order_id', 'customer_id', 'vendor_id', 'chain_id', 'city_id', 'target',\n",
    "            'total_value', 'discount_value', 'delivery_fee', 'user_latitude', 'user_longitude',\n",
    "            'date', 'week', 'discount_perc', 'total_paid_value', 'total_food_value']] \n",
    "\n",
    "vendor_data = (df[['vendor_id', 'chain_id', 'chain_name', 'ddk_flag',\n",
    "                   'vendor_latitude', 'vendor_longitude', 'online_payment', 'accepting_cash',\n",
    "                   'min_delivery_value', 'takeaway_support', 'citymobil_support',\n",
    "                   'default_product_group_id', 'product_group_ids', 'cuisine_ids']]\n",
    "               .drop_duplicates().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сompute behavioural vendor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vendor_features = (\n",
    "    pairs\n",
    "    .pivot_table(index='chain_id', columns='date', values='total_value')\n",
    "    .unstack()\n",
    "    .reset_index()\n",
    "    .sort_values(by=['chain_id', 'date'])\n",
    "    .reset_index()[['chain_id', 'date']]\n",
    ")\n",
    "\n",
    "cols_dct = {\n",
    "    'order_id': 'count',\n",
    "    'total_value': 'sum',\n",
    "    'discount_perc': 'mean',\n",
    "    'total_paid_value': 'sum',\n",
    "    'total_food_value': 'sum'\n",
    "}\n",
    "\n",
    "# daily features\n",
    "date_cols = [f'chain_{col}_{func}' for col, func in cols_dct.items()]\n",
    "date_features = (\n",
    "    pairs\n",
    "    .groupby(['chain_id', 'date'])\n",
    "    .agg(cols_dct)\n",
    "    .reset_index()\n",
    "    .rename(columns=dict(zip(list(cols_dct), date_cols)))\n",
    ")\n",
    "\n",
    "vendor_features = (\n",
    "    vendor_features\n",
    "    .merge(date_features, how='left')\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# yesterday features\n",
    "yesterday_cols = [f'{col}_yesterday' for col in date_cols]\n",
    "vendor_features = pd.concat([\n",
    "    vendor_features,\n",
    "    vendor_features\n",
    "    .groupby('chain_id', sort=False)[date_cols]\n",
    "    .shift(1)\n",
    "    .rename(columns=dict(zip(date_cols, yesterday_cols)))\n",
    "    .reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# rolling features\n",
    "rolling_cols = [f'{col}_rolling14_mean' for col in date_cols]\n",
    "vendor_features = pd.concat([\n",
    "    vendor_features,\n",
    "    vendor_features\n",
    "    .groupby('chain_id', sort=False)[yesterday_cols]\n",
    "    .rolling(14)\n",
    "    .mean()\n",
    "    .rename(columns=dict(zip(yesterday_cols, rolling_cols)))\n",
    "    .reset_index(drop=True)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184500, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain_id</th>\n",
       "      <th>date</th>\n",
       "      <th>chain_order_id_count</th>\n",
       "      <th>chain_total_value_sum</th>\n",
       "      <th>chain_discount_perc_mean</th>\n",
       "      <th>chain_total_paid_value_sum</th>\n",
       "      <th>chain_total_food_value_sum</th>\n",
       "      <th>chain_order_id_count_yesterday</th>\n",
       "      <th>chain_total_value_sum_yesterday</th>\n",
       "      <th>chain_discount_perc_mean_yesterday</th>\n",
       "      <th>chain_total_paid_value_sum_yesterday</th>\n",
       "      <th>chain_total_food_value_sum_yesterday</th>\n",
       "      <th>chain_order_id_count_rolling14_mean</th>\n",
       "      <th>chain_total_value_sum_rolling14_mean</th>\n",
       "      <th>chain_discount_perc_mean_rolling14_mean</th>\n",
       "      <th>chain_total_paid_value_sum_rolling14_mean</th>\n",
       "      <th>chain_total_food_value_sum_rolling14_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chain_id       date  chain_order_id_count  chain_total_value_sum  \\\n",
       "0        26 2020-08-01                   0.0                    0.0   \n",
       "1        26 2020-08-02                   0.0                    0.0   \n",
       "2        26 2020-08-03                   0.0                    0.0   \n",
       "3        26 2020-08-04                   0.0                    0.0   \n",
       "4        26 2020-08-05                   0.0                    0.0   \n",
       "\n",
       "   chain_discount_perc_mean  chain_total_paid_value_sum  \\\n",
       "0                       0.0                         0.0   \n",
       "1                       0.0                         0.0   \n",
       "2                       0.0                         0.0   \n",
       "3                       0.0                         0.0   \n",
       "4                       0.0                         0.0   \n",
       "\n",
       "   chain_total_food_value_sum  chain_order_id_count_yesterday  \\\n",
       "0                         0.0                             NaN   \n",
       "1                         0.0                             0.0   \n",
       "2                         0.0                             0.0   \n",
       "3                         0.0                             0.0   \n",
       "4                         0.0                             0.0   \n",
       "\n",
       "   chain_total_value_sum_yesterday  chain_discount_perc_mean_yesterday  \\\n",
       "0                              NaN                                 NaN   \n",
       "1                              0.0                                 0.0   \n",
       "2                              0.0                                 0.0   \n",
       "3                              0.0                                 0.0   \n",
       "4                              0.0                                 0.0   \n",
       "\n",
       "   chain_total_paid_value_sum_yesterday  chain_total_food_value_sum_yesterday  \\\n",
       "0                                   NaN                                   NaN   \n",
       "1                                   0.0                                   0.0   \n",
       "2                                   0.0                                   0.0   \n",
       "3                                   0.0                                   0.0   \n",
       "4                                   0.0                                   0.0   \n",
       "\n",
       "   chain_order_id_count_rolling14_mean  chain_total_value_sum_rolling14_mean  \\\n",
       "0                                  NaN                                   NaN   \n",
       "1                                  NaN                                   NaN   \n",
       "2                                  NaN                                   NaN   \n",
       "3                                  NaN                                   NaN   \n",
       "4                                  NaN                                   NaN   \n",
       "\n",
       "   chain_discount_perc_mean_rolling14_mean  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "\n",
       "   chain_total_paid_value_sum_rolling14_mean  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   chain_total_food_value_sum_rolling14_mean  \n",
       "0                                        NaN  \n",
       "1                                        NaN  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vendor_features.shape)\n",
    "vendor_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vendor_features.to_parquet('vendor_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vendor_features = pd.read_parquet('vendor_features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сompute behavioural customer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_group_agg_features(data, group_column, value_column, aggregate_functions):\n",
    "    data[f'last_{value_column}'] = data.groupby(group_column)[value_column].shift()\n",
    "    feature_names = [f'{agg_func}_{value_column}' for agg_func in aggregate_functions]\n",
    "    feature_values = data.groupby(group_column)[f'last_{value_column}'].expanding().agg(aggregate_functions).values\n",
    "    data[feature_names] = feature_values\n",
    "\n",
    "def get_onehot_df(order_df, index_names, column_name, prefix):\n",
    "    orders = (\n",
    "        order_df[index_names + [column_name]]\n",
    "        .copy()\n",
    "        .explode(column_name)\n",
    "        .dropna()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    onehot = pd.pivot_table(data=orders,\n",
    "                            index=index_names,\n",
    "                            columns=column_name,\n",
    "                            aggfunc='size')\n",
    "    onehot.columns.name = None\n",
    "    onehot = onehot.reset_index()\n",
    "    key_data = (\n",
    "        order_df[index_names]\n",
    "        .drop_duplicates(keep='first')\n",
    "        .reset_index()\n",
    "        .merge(onehot, on=index_names, how='left')\n",
    "    )\n",
    "    \n",
    "    features = (\n",
    "        key_data\n",
    "        .groupby('customer_id')\n",
    "        .rolling('30d', on='date', closed='left')[onehot.columns[1:]]\n",
    "        .sum()\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    features.columns = [f'cumcount_{prefix}_{col}_roll_30d' for col in onehot.columns[2:]]\n",
    "    features = features.reset_index()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['customer_id', 'date', 'order_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cuisine_ids'] = df['cuisine_ids'].apply(lambda row: eval(row) if pd.notnull(row) else np.nan)\n",
    "df['product_group_ids'] = df['product_group_ids'].apply(lambda row: eval(row) if pd.notnull(row) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dct = {'total_value': ['sum'], \n",
    "           'discount_value': ['sum'], \n",
    "           'delivery_fee': ['sum'], \n",
    "           'discount_perc': ['mean'], \n",
    "           'total_paid_value': ['sum'],\n",
    "           'total_food_value': ['sum'],\n",
    "           'target': ['sum']}\n",
    "\n",
    "\n",
    "customer_group_cols = ['customer_id', 'date']\n",
    "customer_daily_data = df.groupby(customer_group_cols).agg(agg_dct).reset_index()\n",
    "customer_daily_data.columns = customer_daily_data.columns.get_level_values(0)\n",
    "customer_daily_data = customer_daily_data.sort_values(by=customer_group_cols).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_daily_data['days_from_last_order'] = customer_daily_data.groupby('customer_id')['date'].diff().dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_daily_data['cumcount_orders_roll_30d'] = (\n",
    "    customer_daily_data\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30d', on='date', closed='left')['target']\n",
    "    .sum()\n",
    "    .fillna(0)\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_columns = ['total_value', 'discount_value', 'delivery_fee',\n",
    "                     'discount_perc', 'total_paid_value', 'total_food_value']\n",
    "aggregate_functions = ['min', 'mean', 'median', 'max']\n",
    "group_column = 'customer_id'\n",
    "time_column = 'date'\n",
    "\n",
    "customer_rolling_data = (\n",
    "    customer_daily_data\n",
    "    .groupby(group_column)\n",
    "    .rolling('30d', on='date', closed='left')[aggregate_columns]\n",
    "    .agg(aggregate_functions)\n",
    "    .fillna(0)\n",
    ").values\n",
    "\n",
    "zip_col_names = list(itertools.product(aggregate_columns, aggregate_functions))\n",
    "customer_rolling_columns = [f'{val}_{agg}_roll_30d' for val, agg in zip_col_names]\n",
    "customer_daily_data[customer_rolling_columns] = customer_rolling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = ['customer_id', 'date']\n",
    "column_name = 'cuisine_ids'\n",
    "prefix = 'cuisine_id'\n",
    "\n",
    "cuisine_onehot_features = get_onehot_df(df, index_names, column_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'product_group_ids'\n",
    "prefix = 'product_group_id'\n",
    "\n",
    "product_onehot_features = get_onehot_df(df, index_names, column_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = (\n",
    "    customer_daily_data\n",
    "    .merge(cuisine_onehot_features, on=index_names, how='left')\n",
    "    .merge(product_onehot_features, on=index_names, how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = customer_features[\n",
    "    [col for col in customer_features.columns if col not in aggregate_columns + ['target']]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557007, 78)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>days_from_last_order</th>\n",
       "      <th>cumcount_orders_roll_14d</th>\n",
       "      <th>total_value_min_roll_14d</th>\n",
       "      <th>total_value_mean_roll_14d</th>\n",
       "      <th>total_value_median_roll_14d</th>\n",
       "      <th>total_value_max_roll_14d</th>\n",
       "      <th>discount_value_min_roll_14d</th>\n",
       "      <th>discount_value_mean_roll_14d</th>\n",
       "      <th>discount_value_median_roll_14d</th>\n",
       "      <th>discount_value_max_roll_14d</th>\n",
       "      <th>delivery_fee_min_roll_14d</th>\n",
       "      <th>delivery_fee_mean_roll_14d</th>\n",
       "      <th>delivery_fee_median_roll_14d</th>\n",
       "      <th>delivery_fee_max_roll_14d</th>\n",
       "      <th>discount_perc_min_roll_14d</th>\n",
       "      <th>discount_perc_mean_roll_14d</th>\n",
       "      <th>discount_perc_median_roll_14d</th>\n",
       "      <th>discount_perc_max_roll_14d</th>\n",
       "      <th>total_paid_value_min_roll_14d</th>\n",
       "      <th>total_paid_value_mean_roll_14d</th>\n",
       "      <th>total_paid_value_median_roll_14d</th>\n",
       "      <th>total_paid_value_max_roll_14d</th>\n",
       "      <th>total_food_value_min_roll_14d</th>\n",
       "      <th>total_food_value_mean_roll_14d</th>\n",
       "      <th>total_food_value_median_roll_14d</th>\n",
       "      <th>total_food_value_max_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_1_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_2_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_3_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_4_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_5_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_6_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_22_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_23_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_25_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_26_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_28_roll_14d</th>\n",
       "      <th>cumcount_cuisine_id_29_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_1_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_2_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_3_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_5_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_6_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_18_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_19_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_20_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_21_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_22_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_23_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_24_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_25_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_26_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_27_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_28_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_29_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_30_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_31_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_32_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_33_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_34_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_35_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_36_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_37_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_38_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_39_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_40_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_41_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_42_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_43_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_44_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_45_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_47_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_48_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_49_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_51_roll_14d</th>\n",
       "      <th>cumcount_product_group_id_52_roll_14d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>2020-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>2020-10-13</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>246.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>219</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>219</td>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id       date  days_from_last_order  cumcount_orders_roll_14d  \\\n",
       "0           86 2020-10-03                   NaN                       0.0   \n",
       "1           86 2020-10-13                  10.0                       1.0   \n",
       "2          219 2020-08-13                   NaN                       0.0   \n",
       "3          219 2020-10-05                  53.0                       0.0   \n",
       "4          350 2020-10-07                   NaN                       0.0   \n",
       "\n",
       "   total_value_min_roll_14d  total_value_mean_roll_14d  \\\n",
       "0                       0.0                        0.0   \n",
       "1                     427.0                      427.0   \n",
       "2                       0.0                        0.0   \n",
       "3                       0.0                        0.0   \n",
       "4                       0.0                        0.0   \n",
       "\n",
       "   total_value_median_roll_14d  total_value_max_roll_14d  \\\n",
       "0                          0.0                       0.0   \n",
       "1                        427.0                     427.0   \n",
       "2                          0.0                       0.0   \n",
       "3                          0.0                       0.0   \n",
       "4                          0.0                       0.0   \n",
       "\n",
       "   discount_value_min_roll_14d  discount_value_mean_roll_14d  \\\n",
       "0                          0.0                           0.0   \n",
       "1                         82.0                          82.0   \n",
       "2                          0.0                           0.0   \n",
       "3                          0.0                           0.0   \n",
       "4                          0.0                           0.0   \n",
       "\n",
       "   discount_value_median_roll_14d  discount_value_max_roll_14d  \\\n",
       "0                             0.0                          0.0   \n",
       "1                            82.0                         82.0   \n",
       "2                             0.0                          0.0   \n",
       "3                             0.0                          0.0   \n",
       "4                             0.0                          0.0   \n",
       "\n",
       "   delivery_fee_min_roll_14d  delivery_fee_mean_roll_14d  \\\n",
       "0                        0.0                         0.0   \n",
       "1                       99.0                        99.0   \n",
       "2                        0.0                         0.0   \n",
       "3                        0.0                         0.0   \n",
       "4                        0.0                         0.0   \n",
       "\n",
       "   delivery_fee_median_roll_14d  delivery_fee_max_roll_14d  \\\n",
       "0                           0.0                        0.0   \n",
       "1                          99.0                       99.0   \n",
       "2                           0.0                        0.0   \n",
       "3                           0.0                        0.0   \n",
       "4                           0.0                        0.0   \n",
       "\n",
       "   discount_perc_min_roll_14d  discount_perc_mean_roll_14d  \\\n",
       "0                        0.00                         0.00   \n",
       "1                        0.25                         0.25   \n",
       "2                        0.00                         0.00   \n",
       "3                        0.00                         0.00   \n",
       "4                        0.00                         0.00   \n",
       "\n",
       "   discount_perc_median_roll_14d  discount_perc_max_roll_14d  \\\n",
       "0                           0.00                        0.00   \n",
       "1                           0.25                        0.25   \n",
       "2                           0.00                        0.00   \n",
       "3                           0.00                        0.00   \n",
       "4                           0.00                        0.00   \n",
       "\n",
       "   total_paid_value_min_roll_14d  total_paid_value_mean_roll_14d  \\\n",
       "0                            0.0                             0.0   \n",
       "1                          246.0                           246.0   \n",
       "2                            0.0                             0.0   \n",
       "3                            0.0                             0.0   \n",
       "4                            0.0                             0.0   \n",
       "\n",
       "   total_paid_value_median_roll_14d  total_paid_value_max_roll_14d  \\\n",
       "0                               0.0                            0.0   \n",
       "1                             246.0                          246.0   \n",
       "2                               0.0                            0.0   \n",
       "3                               0.0                            0.0   \n",
       "4                               0.0                            0.0   \n",
       "\n",
       "   total_food_value_min_roll_14d  total_food_value_mean_roll_14d  \\\n",
       "0                            0.0                             0.0   \n",
       "1                          328.0                           328.0   \n",
       "2                            0.0                             0.0   \n",
       "3                            0.0                             0.0   \n",
       "4                            0.0                             0.0   \n",
       "\n",
       "   total_food_value_median_roll_14d  total_food_value_max_roll_14d  \\\n",
       "0                               0.0                            0.0   \n",
       "1                             328.0                          328.0   \n",
       "2                               0.0                            0.0   \n",
       "3                               0.0                            0.0   \n",
       "4                               0.0                            0.0   \n",
       "\n",
       "   cumcount_cuisine_id_1_roll_14d  cumcount_cuisine_id_2_roll_14d  \\\n",
       "0                             0.0                             0.0   \n",
       "1                             0.0                             1.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             0.0   \n",
       "\n",
       "   cumcount_cuisine_id_3_roll_14d  cumcount_cuisine_id_4_roll_14d  \\\n",
       "0                             0.0                             0.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             0.0   \n",
       "\n",
       "   cumcount_cuisine_id_5_roll_14d  cumcount_cuisine_id_6_roll_14d  \\\n",
       "0                             0.0                             0.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             0.0   \n",
       "\n",
       "   cumcount_cuisine_id_22_roll_14d  cumcount_cuisine_id_23_roll_14d  \\\n",
       "0                              0.0                              0.0   \n",
       "1                              0.0                              0.0   \n",
       "2                              0.0                              0.0   \n",
       "3                              0.0                              0.0   \n",
       "4                              0.0                              0.0   \n",
       "\n",
       "   cumcount_cuisine_id_25_roll_14d  cumcount_cuisine_id_26_roll_14d  \\\n",
       "0                              0.0                              0.0   \n",
       "1                              0.0                              0.0   \n",
       "2                              0.0                              0.0   \n",
       "3                              0.0                              0.0   \n",
       "4                              0.0                              0.0   \n",
       "\n",
       "   cumcount_cuisine_id_28_roll_14d  cumcount_cuisine_id_29_roll_14d  \\\n",
       "0                              0.0                              0.0   \n",
       "1                              0.0                              0.0   \n",
       "2                              0.0                              0.0   \n",
       "3                              0.0                              0.0   \n",
       "4                              0.0                              0.0   \n",
       "\n",
       "   cumcount_product_group_id_1_roll_14d  cumcount_product_group_id_2_roll_14d  \\\n",
       "0                                   0.0                                   0.0   \n",
       "1                                   0.0                                   1.0   \n",
       "2                                   0.0                                   0.0   \n",
       "3                                   0.0                                   0.0   \n",
       "4                                   0.0                                   0.0   \n",
       "\n",
       "   cumcount_product_group_id_3_roll_14d  cumcount_product_group_id_5_roll_14d  \\\n",
       "0                                   0.0                                   0.0   \n",
       "1                                   0.0                                   0.0   \n",
       "2                                   0.0                                   0.0   \n",
       "3                                   0.0                                   0.0   \n",
       "4                                   0.0                                   0.0   \n",
       "\n",
       "   cumcount_product_group_id_6_roll_14d  \\\n",
       "0                                   0.0   \n",
       "1                                   0.0   \n",
       "2                                   0.0   \n",
       "3                                   0.0   \n",
       "4                                   0.0   \n",
       "\n",
       "   cumcount_product_group_id_18_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_19_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_20_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    1.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_21_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_22_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_23_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_24_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_25_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_26_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_27_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_28_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_29_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_30_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_31_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_32_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_33_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_34_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_35_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_36_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    1.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_37_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_38_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_39_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_40_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_41_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_42_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_43_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_44_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_45_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_47_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_48_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_49_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_51_roll_14d  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   cumcount_product_group_id_52_roll_14d  \n",
       "0                                    0.0  \n",
       "1                                    0.0  \n",
       "2                                    0.0  \n",
       "3                                    0.0  \n",
       "4                                    0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(customer_features.shape)\n",
    "customer_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# customer_features.to_parquet('customer_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_features = pd.read_parquet('customer_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.week.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_folds(df):\n",
    "    # split data on train and validation\n",
    "    folds = {\n",
    "        'test': df[df['week'] >= 11].index.tolist(),\n",
    "        'train': df[(df['week'] >= 4) & (df['week'] < 11)].index.tolist(),\n",
    "    }\n",
    "    \n",
    "    # train - 4 weeks, val - 1 week\n",
    "    for fold_num, week in enumerate(range(0, 7, 3)):\n",
    "        fold_train = df[(df['week'] >= week) & (df['week'] < (week + 4))]\n",
    "        folds[f'train_{fold_num + 1}'] = fold_train.index.values\n",
    "        \n",
    "        fold_val = df[(df['week'] == (week + 4)) \n",
    "                      & (~df['chain_id'].isin(fastfoold_chains))] # remove fastfood from validation\n",
    "        \n",
    "        fold_val_index = fold_val.index.values\n",
    "            \n",
    "        # remove train orders from validation\n",
    "        tmp = fold_train[['customer_id', 'chain_id']].drop_duplicates()\n",
    "        tmp['is_in_train'] = 1\n",
    "        fold_val = fold_val.merge(tmp, how='left')\n",
    "        \n",
    "        folds[f'val_{fold_num + 1}'] = fold_val_index[fold_val['is_in_train'].isnull()]\n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vendor_tree(df):\n",
    "    # bulld data structure for fast geo search\n",
    "    vendor_coords = df[['vendor_id', 'vendor_latitude', 'vendor_longitude']].drop_duplicates()\n",
    "    \n",
    "    tree = BallTree(np.radians(vendor_coords[['vendor_latitude', 'vendor_longitude']]), \n",
    "                    leaf_size=40, metric='haversine')\n",
    "    \n",
    "    return tree, vendor_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_zeros(tree, pairs, vendor_coords, radius=5000, n_samples=100):\n",
    "    # sample zeros as vendor_ids within some radius\n",
    "    R = 6367000\n",
    "    ind, dist = tree.query_radius(\n",
    "        np.radians(pairs[['user_latitude', 'user_longitude']]), r=radius/R, return_distance=True)\n",
    "    \n",
    "    zeros = []\n",
    "    vendor_coords_values = vendor_coords['vendor_id'].values\n",
    "    cols = ['customer_id', 'date', 'user_latitude', 'user_longitude', 'city_id']\n",
    "    \n",
    "    for i, (customer_id, date, lat, lon, city) in enumerate(pairs[cols].values):\n",
    "        vendors = ind[i]\n",
    "        if n_samples:\n",
    "            vendors = np.random.choice(ind[i], size=min(len(vendors), n_samples), replace=False)\n",
    "        zeros += [(customer_id, vendor, date, lat, lon, city, 0) for vendor in vendor_coords_values[vendors]]\n",
    "        \n",
    "    return pd.DataFrame(zeros, columns=['customer_id', 'vendor_id', 'date',\n",
    "                                        'user_latitude', 'user_longitude', 'city_id', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_folds(fold, pairs_zeros):\n",
    "    \n",
    "    fold = pd.concat([fold, \n",
    "                      pairs_zeros[pairs_zeros['customer_id'].isin(fold['customer_id'])\n",
    "                                 & (pairs_zeros['date'] >= fold['date'].min())\n",
    "                                 & (pairs_zeros['date'] <= fold['date'].max())]], axis=0)\n",
    "    fold.drop_duplicates(subset=['customer_id', 'chain_id', 'target'], inplace=True)\n",
    "    fold = (fold.groupby(['customer_id', 'chain_id'])['target'].max().reset_index()\n",
    "            .merge(fold))\n",
    "    return fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree, vendor_coords = build_vendor_tree(vendor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_zeros_train = sample_zeros(tree, pairs, vendor_coords, n_samples=100)\n",
    "\n",
    "# pairs_zeros_val = sample_zeros(tree, customer_data, vendor_coords, n_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chain_id to sampled zeros (vendor_ids)\n",
    "pairs_zeros_train = pairs_zeros_train.merge(vendor_data[['vendor_id', 'chain_id']].drop_duplicates())\n",
    "\n",
    "# pairs_zeros_val = (pairs_zeros_val\n",
    "#                .merge(vendor_data[['vendor_id', 'chain_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49713195, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_latitude</th>\n",
       "      <th>user_longitude</th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "      <th>chain_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15955880</td>\n",
       "      <td>267406</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>55.7815</td>\n",
       "      <td>37.5307</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85014577</td>\n",
       "      <td>267406</td>\n",
       "      <td>2020-08-03</td>\n",
       "      <td>55.7713</td>\n",
       "      <td>37.5840</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14636867</td>\n",
       "      <td>267406</td>\n",
       "      <td>2020-08-04</td>\n",
       "      <td>55.7347</td>\n",
       "      <td>37.4908</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65920510</td>\n",
       "      <td>267406</td>\n",
       "      <td>2020-09-12</td>\n",
       "      <td>55.7347</td>\n",
       "      <td>37.4908</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76331931</td>\n",
       "      <td>267406</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>55.7498</td>\n",
       "      <td>37.5903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  vendor_id       date  user_latitude  user_longitude  city_id  \\\n",
       "0     15955880     267406 2020-08-01        55.7815         37.5307        1   \n",
       "1     85014577     267406 2020-08-03        55.7713         37.5840        1   \n",
       "2     14636867     267406 2020-08-04        55.7347         37.4908        1   \n",
       "3     65920510     267406 2020-09-12        55.7347         37.4908        1   \n",
       "4     76331931     267406 2020-10-06        55.7498         37.5903        1   \n",
       "\n",
       "   target  chain_id  \n",
       "0       0     50738  \n",
       "1       0     50738  \n",
       "2       0     50738  \n",
       "3       0     50738  \n",
       "4       0     50738  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pairs_zeros_train.shape)\n",
    "pairs_zeros_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = split_by_folds(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_zeros_train.to_parquet('pairs_zeros_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_zeros_train = pd.read_parquet('pairs_zeros_train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import node2vec\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEmbeddings:\n",
    "    def __init__(self, df, dims, threshold):\n",
    "        self.df = df[['customer_id', 'chain_id']].copy()\n",
    "        self.dims = dims\n",
    "        self.threshold = threshold\n",
    "        self.G = None\n",
    "        self.model = None\n",
    "        \n",
    "    def _prepare_graph(self):\n",
    "        print('start graph construction')\n",
    "        latch = TimeLatch()\n",
    "        group = self.df.groupby('customer_id').chain_id.apply(set)\n",
    "        edges = defaultdict(int)\n",
    "        for chain_ids in tqdm(group.values):\n",
    "            for pair in itertools.combinations(chain_ids, 2):\n",
    "                edges[tuple(sorted(pair))] += 1\n",
    "        print(f'number of edges: {len(edges)}')\n",
    "        p = np.percentile(np.array(list(edges.values())), [0.01, 0.05, 0.1])\n",
    "        print(f'weight percentiles: 0.01 - {p[0]}; 0.05 - {p[1]}; 0.1 - {p[2]}')\n",
    "        self.G = nx.Graph()\n",
    "        for k, v in  edges.items():\n",
    "            if v >= self.threshold:\n",
    "                self.G.add_edge(*k, weight=v)\n",
    "        print(f'graph constructed, took {latch.took_msg()}')\n",
    "        print(nx.info(self.G))\n",
    "            \n",
    "    def fit(self):\n",
    "        self._prepare_graph()\n",
    "        print('start fitting the model')\n",
    "        latch = TimeLatch()\n",
    "        n2v = node2vec.Node2Vec(self.G, dimensions=self.dims)\n",
    "        self.model = n2v.fit()\n",
    "        print(f'model fit, took {latch.took_msg()}')\n",
    "        \n",
    "    def transform(self, df):\n",
    "        idx = np.array(self.model.wv.index2word, dtype=int)\n",
    "        idx = pd.Series(index=idx, data=np.arange(len(idx)), name='idx')\n",
    "        idx = pd.merge(df['chain_id'], idx, left_on='chain_id', right_index=True, how='left')\n",
    "        vecs = np.vstack([self.model.wv.vectors, np.zeros(shape=(1,self.dims), dtype=np.float)])\n",
    "        idx.loc[np.isnan(idx['idx']), 'idx'] = vecs.shape[0] - 1\n",
    "        return vecs[idx['idx'].values.astype(np.int32)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_graph_train = GraphEmbeddings(pairs[pairs.week < pairs.loc[folds['train']].week.min()], threshold=0, dims=64)\n",
    "# emb_graph_train.fit()\n",
    "\n",
    "emb_graph_test = GraphEmbeddings(pairs[(pairs.week < pairs.loc[folds['test']].week.min()) & \n",
    "                                    (pairs.week >= pairs.loc[folds['test']].week.min() - 4)], threshold=0, dims=64)\n",
    "emb_graph_test.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(l, size):\n",
    "    h = np.zeros(size + 1, dtype=np.float32)\n",
    "    if isinstance(l, str):\n",
    "        l = [int(x.replace(',', '')) for x in l[1:-1].split()]\n",
    "    h[l] = 1.\n",
    "    return h\n",
    "\n",
    "def hist_graph_emb_mean(df, emb_graph):\n",
    "    cumemb = emb_graph.transform(df).cumsum(axis=0)\n",
    "    cumemb = cumemb[:-1] / np.arange(1, df.shape[0])[:,None]\n",
    "    return np.vstack([np.zeros(shape=(1, emb_graph.dims), dtype=np.float), cumemb])  \n",
    "\n",
    "def hist_product_ids_mean(df, emb_graph):\n",
    "    cumemb = np.vstack(df['product_group_ids'].apply(partial(to_onehot, size=52)).values).cumsum(axis=0)\n",
    "    cumemb = cumemb[:-1] / np.arange(1, df.shape[0])[:,None]\n",
    "    return np.vstack([np.zeros(shape=(1, 53), dtype=np.float), cumemb])  \n",
    "\n",
    "\n",
    "def foo(df):    \n",
    "    ohe = np.vstack(df['product_group_ids'].apply(partial(to_onehot, size=52)).values)\n",
    "    cols = [f'col_{i}' for i in range(ohe.shape[1])]\n",
    "    df[cols] = ohe\n",
    "    x = df.rolling('30d', on='date', closed='left')[cols].mean()\n",
    "    return x[cols].values\n",
    "\n",
    "\n",
    "def hist_graph_emb_sum(df, emb_graph):\n",
    "    cumemb = emb_graph.transform(df).cumsum(axis=0)\n",
    "    cumemb = cumemb[:-1] / np.arange(1, df.shape[0])[:,None]\n",
    "    return np.vstack([np.zeros(shape=(1, emb_graph.dims), dtype=np.float), cumemb])  \n",
    "\n",
    "def _compute_historical_feature(feature_computer, df, groups):\n",
    "    values = [\n",
    "        feature_computer(group[0]) for group in _group(groups, df)\n",
    "    ]\n",
    "    return np.vstack(values)\n",
    "\n",
    "def compute_historical_feature(feature_computer, df, ncores):   \n",
    "    groups = get_serp_sizes(df['customer_id'])\n",
    "    groups_parts = partition(groups, ncores)\n",
    "    features = Parallel(n_jobs=ncores)(\n",
    "        delayed(_compute_historical_feature)(feature_computer, df_part[0], groups_part)\n",
    "        for df_part, groups_part in zip(_group(map(sum, groups_parts), df), groups_parts)\n",
    "    )    \n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_trxain = compute_historical_feature(partial(hist_graph_emb_mean, emb_graph=emb_graph_train), df, 36)\n",
    "user_emb_test = compute_historical_feature(partial(hist_graph_emb_mean, emb_graph=emb_graph_test), df, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_hist_ohe_products = compute_historical_feature(foo, df, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_index = df[['customer_id', 'date']].reset_index().copy()\n",
    "user_emb_index.columns = ['num', 'customer_id', 'date']\n",
    "user_emb_index = user_emb_index.groupby(['customer_id', 'date'])['num'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_graph_emp_hist_features(df, emb_graph, user_emb, user_emb_index):    \n",
    "    user_vals = user_emb[df.merge(user_emb_index, on=['customer_id', 'date']).num]\n",
    "    \n",
    "    df['cos_hist_graph_emb_mean'] = cosine(user_vals, emb_graph.transform(df))\n",
    "    df['bc_hist_graph_emb_mean'] = braycurtis(user_vals, emb_graph.transform(df))\n",
    "    df['dot_hist_graph_emb_mean'] = dot(user_vals, emb_graph.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_products_ohe_features(df, user_emb, user_emb_index):    \n",
    "    user_vals = user_emb[df.merge(user_emb_index, on=['customer_id', 'date']).num]\n",
    "    chain_vals = np.vstack(df['product_group_ids'].apply(partial(to_onehot, size=52)).values)\n",
    "    \n",
    "    df['cos_hist_products_ohe_mean'] = cosine(user_vals, chain_vals)\n",
    "    df['bc_hist_products_ohe_mean'] = braycurtis(user_vals, chain_vals)\n",
    "    df['dot_hist_products_ohe_mean'] = dot(user_vals, chain_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367000 * c\n",
    "    return km\n",
    "\n",
    "def create_features(pairs, vendor_data, vendor_features, customer_features):\n",
    "    # add source vendor features\n",
    "    pairs = pairs.merge(vendor_data, on=['chain_id', 'vendor_id'])\n",
    "\n",
    "    # add vendor behavioural features\n",
    "    pairs = pairs.merge(vendor_features, on=['chain_id', 'date'])\n",
    "\n",
    "    # add customer behavioural features\n",
    "    pairs = pairs.merge(customer_features, on=['customer_id', 'date'])\n",
    "\n",
    "    # add simple features\n",
    "    pairs['distance'] = haversine(pairs['user_latitude'], pairs['user_longitude'],\n",
    "                                  pairs['vendor_latitude'], pairs['vendor_longitude']).round(-2)\n",
    "    pairs['min_delivery_value'] = pairs['min_delivery_value'].round(-3)\n",
    "    pairs['day_of_week'] = pairs['date'].dt.dayofweek\n",
    "\n",
    "    # sort data for lambdamart\n",
    "    pairs.sort_values(by=['customer_id', 'chain_id', 'target'], inplace=True)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_feats = [col for col in customer_features.columns if col not in ['customer_id', 'date']]\n",
    "\n",
    "customer_feats = [\n",
    "#  'days_from_last_order', # надо бы ограничить окном\n",
    " 'cumcount_orders_roll_30d',\n",
    " 'total_value_min_roll_30d',\n",
    " 'total_value_mean_roll_30d',\n",
    " 'total_value_median_roll_30d',\n",
    " 'total_value_max_roll_30d',\n",
    " 'discount_value_min_roll_30d',\n",
    " 'discount_value_mean_roll_30d',\n",
    " 'discount_value_median_roll_30d',\n",
    " 'discount_value_max_roll_30d',\n",
    "]\n",
    "\n",
    "product_id_ohe_features = [\n",
    "    'cos_hist_products_ohe_mean',\n",
    "    'bc_hist_products_ohe_mean',\n",
    "    'dot_hist_products_ohe_mean',\n",
    "]\n",
    "\n",
    "graph_emb_features = [\n",
    "    'cos_hist_graph_emb_mean',\n",
    "    'bc_hist_graph_emb_mean',\n",
    "    'dot_hist_graph_emb_mean'\n",
    "]\n",
    "\n",
    "feats = [\n",
    "    # source order features\n",
    "    'user_latitude', 'user_longitude', \n",
    "    'city_id', \n",
    "    \n",
    "    # source vendor features\n",
    "    'vendor_latitude', 'vendor_longitude', \n",
    "    'ddk_flag', 'online_payment', 'accepting_cash',\n",
    "    'min_delivery_value', 'takeaway_support', 'citymobil_support', 'default_product_group_id', \n",
    "    \n",
    "    # behavioural vendor features\n",
    "    'chain_order_id_count_yesterday',\n",
    "    'chain_total_value_sum_yesterday', \n",
    "    'chain_discount_perc_mean_yesterday',\n",
    "    'chain_total_paid_value_sum_yesterday',\n",
    "    'chain_total_food_value_sum_yesterday',\n",
    "    'chain_order_id_count_rolling14_mean',\n",
    "    'chain_total_value_sum_rolling14_mean',\n",
    "    'chain_discount_perc_mean_rolling14_mean',\n",
    "    'chain_total_paid_value_sum_rolling14_mean',\n",
    "    'chain_total_food_value_sum_rolling14_mean',\n",
    "    \n",
    "    # simple features\n",
    "    'distance',\n",
    "    'day_of_week'\n",
    "] \n",
    "\n",
    "# feats += product_id_ohe_features\n",
    "feats += customer_feats  \n",
    "feats += graph_emb_features\n",
    "# feats += list(filter(lambda x: x in customer_features.columns, [f'cumcount_product_group_id_{i}_roll_30d' for i in range(53)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fold(pairs, pairs_zeros, vendor_features, customer_features, distance=5000):\n",
    "    pairs = add_zeros_to_folds(pairs, pairs_zeros)\n",
    "    pairs = create_features(pairs, vendor_data, vendor_features, customer_features)\n",
    "    \n",
    "    # remove target with distance greater than sampled for train\n",
    "    pairs = pairs[pairs['distance'] < distance]\n",
    "    pairs = pairs[pairs.groupby('customer_id')['target'].transform('sum') > 0]\n",
    "    add_products_ohe_features(pairs, user_hist_ohe_products, user_emb_index)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on fold 3\n",
    "pairs_feats = pairs_zeros_train.columns.tolist() \n",
    "\n",
    "# train = prepare_fold(pairs.loc[folds['train'], pairs_feats], pairs_zeros_train, vendor_features, customer_features)\n",
    "test = prepare_fold(pairs.loc[folds['test'], pairs_feats], pairs_zeros_train, vendor_features, customer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f26ce0862e8>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_graph_test.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_graph_emp_hist_features(train, emb_graph_train, user_emb_train, user_emb_index)\n",
    "add_graph_emp_hist_features(test, emb_graph_test, user_emb_test, user_emb_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'num_boost_round': 1000,\n",
    "    'early_stopping_round': 50,\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'eval_at': [15],\n",
    "    'verbosity': 1,\n",
    "#     'max_position': 15,\n",
    "    'lambdamart_norm': True,\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 8,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'scale_pos_weight': 1,\n",
    "    'seed': 42,\n",
    "    'nthread': 36\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train[feats], label=train['target'],\n",
    "                     group=train.groupby('customer_id')['target'].count().values)\n",
    "dval = lgb.Dataset(test[feats], label=test['target'],\n",
    "                   group=test.groupby('customer_id')['target'].count().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lambdamart_norm\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: lambdamart_norm\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.388436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6533\n",
      "[LightGBM] [Info] Number of data points in the train set: 16427174, number of used features: 36\n",
      "[LightGBM] [Warning] Unknown parameter: lambdamart_norm\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's ndcg@15: 0.538673\tvalid_1's ndcg@15: 0.463722\n",
      "[100]\ttraining's ndcg@15: 0.547848\tvalid_1's ndcg@15: 0.473075\n",
      "[150]\ttraining's ndcg@15: 0.554006\tvalid_1's ndcg@15: 0.478426\n",
      "[200]\ttraining's ndcg@15: 0.55832\tvalid_1's ndcg@15: 0.4811\n",
      "[250]\ttraining's ndcg@15: 0.561615\tvalid_1's ndcg@15: 0.482023\n",
      "[300]\ttraining's ndcg@15: 0.56417\tvalid_1's ndcg@15: 0.483435\n",
      "[350]\ttraining's ndcg@15: 0.56624\tvalid_1's ndcg@15: 0.483794\n",
      "[400]\ttraining's ndcg@15: 0.568596\tvalid_1's ndcg@15: 0.484909\n",
      "[450]\ttraining's ndcg@15: 0.570578\tvalid_1's ndcg@15: 0.485582\n",
      "[500]\ttraining's ndcg@15: 0.573249\tvalid_1's ndcg@15: 0.486929\n",
      "[550]\ttraining's ndcg@15: 0.575032\tvalid_1's ndcg@15: 0.48774\n",
      "[600]\ttraining's ndcg@15: 0.577201\tvalid_1's ndcg@15: 0.488414\n",
      "[650]\ttraining's ndcg@15: 0.578789\tvalid_1's ndcg@15: 0.48873\n",
      "[700]\ttraining's ndcg@15: 0.580539\tvalid_1's ndcg@15: 0.489107\n",
      "[750]\ttraining's ndcg@15: 0.582334\tvalid_1's ndcg@15: 0.489524\n",
      "[800]\ttraining's ndcg@15: 0.583989\tvalid_1's ndcg@15: 0.489663\n",
      "[850]\ttraining's ndcg@15: 0.585229\tvalid_1's ndcg@15: 0.489965\n",
      "Early stopping, best iteration is:\n",
      "[845]\ttraining's ndcg@15: 0.585161\tvalid_1's ndcg@15: 0.489995\n"
     ]
    }
   ],
   "source": [
    "evals_res = {}\n",
    "model = lgb.train(lgb_params, dtrain, valid_sets=[dtrain, dval], evals_result=evals_res, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||MAUC|Recall@15|NDCG@15|DIV_PROD@15|DIV_CHAIN@15|COV_PROD@15|COV_CHAIN@15|\n",
    "|:---|---|---|---|---|---|---|---|\n",
    "|RAND|0.4993|0.2133|0.0883|2.8941|8.9504|37|12831|\n",
    "|ALS_BASELINE|0.6874|0.4706|0.2847|**2.8863**|**8.3969**|37|**12168**|\n",
    "|LGBM_BASELINE|0.8434|0.7049|0.4185|2.5852|8.1240|37|11783|\n",
    "|LGBM_HIST_FEATS|**0.8713**|0.7661|0.4635|2.6722|7.6576|37|10694|\n",
    "|LGBM_GRAPH_EMB|0.8694|**0.7667**|**0.4897**|2.7067|7.7415|37|10970|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauc: 0.6874064455565415\n",
      "ndcg@1: 0.16570459333023282\n",
      "recall@1: 0.16570459333023282\n",
      "entropy_diversity_of_chain_id@1: 7.420061839135916\n",
      "entropy_diversity_of_default_product_group_id@1: 2.6330890419294146\n",
      "coverage_of_chain_id@1: 8929\n",
      "coverage_of_default_product_group_id@1: 37\n",
      "ndcg@5: 0.23195645450048313\n",
      "recall@5: 0.299377616064345\n",
      "entropy_diversity_of_chain_id@5: 8.03734504948276\n",
      "entropy_diversity_of_default_product_group_id@5: 2.8255788402898427\n",
      "coverage_of_chain_id@5: 11652\n",
      "coverage_of_default_product_group_id@5: 37\n",
      "ndcg@15: 0.28478177355730866\n",
      "recall@15: 0.47062292457266874\n",
      "entropy_diversity_of_chain_id@15: 8.39695631534089\n",
      "entropy_diversity_of_default_product_group_id@15: 2.8863908090413726\n",
      "coverage_of_chain_id@15: 12168\n",
      "coverage_of_default_product_group_id@15: 37\n"
     ]
    }
   ],
   "source": [
    "# complete randomness\n",
    "# test['pred'] = np.random.rand(test.shape[0])\n",
    "test['pred'] = -np.array(pred)\n",
    "test[test['pred'] == -1]['pred'] = -10\n",
    "for metric in METRICS:\n",
    "    print(f'{metric.name}: {metric(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauc: 0.8694278330255876\n",
      "ndcg@1: 0.30703362241129317\n",
      "recall@1: 0.30703362241129317\n",
      "entropy_diversity_of_chain_id@1: 6.561309077447194\n",
      "entropy_diversity_of_default_product_group_id@1: 2.541625110939868\n",
      "coverage_of_chain_id@1: 5090\n",
      "coverage_of_default_product_group_id@1: 36\n",
      "ndcg@5: 0.4149890478739871\n",
      "recall@5: 0.528819339953857\n",
      "entropy_diversity_of_chain_id@5: 7.1272198830916205\n",
      "entropy_diversity_of_default_product_group_id@5: 2.6455748817781193\n",
      "coverage_of_chain_id@5: 8181\n",
      "coverage_of_default_product_group_id@5: 37\n",
      "ndcg@15: 0.4897016511106964\n",
      "recall@15: 0.7667022965999795\n",
      "entropy_diversity_of_chain_id@15: 7.7415090232035055\n",
      "entropy_diversity_of_default_product_group_id@15: 2.7067631353542874\n",
      "coverage_of_chain_id@15: 10970\n",
      "coverage_of_default_product_group_id@15: 37\n"
     ]
    }
   ],
   "source": [
    "# model on all features \n",
    "test['pred'] = model.predict(test[feats])\n",
    "for metric in METRICS:\n",
    "    print(f'{metric.name}: {metric(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2730224, 117)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_components': 100,\n",
    "    'bm25_params': {\n",
    "        'K1': 100,\n",
    "        'B': 0.8,\n",
    "    },\n",
    "    'seed': 42,\n",
    "    'scale': 100,\n",
    "    'num_threads': 2,\n",
    "    'num_iter': 10,\n",
    "    'regularization': 1\n",
    "}\n",
    "\n",
    "def create_item_user_matrix(df, params):\n",
    "    data = df[['customer_id', 'chain_id', 'target']].copy()\n",
    "    \n",
    "    unique_rows = list(np.unique(data['customer_id']).astype(np.int32))\n",
    "    unique_cols = list(np.unique(data['chain_id']).astype(np.int32))\n",
    "\n",
    "    row_map = dict(zip(unique_rows, range(len(unique_rows))))\n",
    "    col_map = dict(zip(unique_cols, range(len(unique_cols))))\n",
    "\n",
    "    data['customer_id'] = data['customer_id'].map(row_map)\n",
    "    data['chain_id'] = data['chain_id'].map(col_map)\n",
    "\n",
    "    matrix = csr_matrix((data['target'], [data['customer_id'], data['chain_id']]))\n",
    "    matrix = bm25_weight(matrix, **params['bm25_params']).T.tocsr() * params['scale']\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def compute_als_decomposition(matrix, params):\n",
    "    np.random.seed(params['seed'])\n",
    "    implicit_als = implicit.als.AlternatingLeastSquares(\n",
    "        params['n_components'], num_threads=params['num_threads'], iterations=params['num_iter'],\n",
    "        regularization=params['regularization']\n",
    "    )\n",
    "    implicit_als.fit(matrix, show_progress=False)\n",
    "\n",
    "    return implicit_als"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_fold = pairs.loc[folds['train']].groupby(['customer_id', 'chain_id'])['target'].max().reset_index()\n",
    "prev_matrix = create_item_user_matrix(previous_fold, params)\n",
    "\n",
    "train_m, val_m = train_test_split(prev_matrix, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import implicit\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.evaluation import train_test_split, ndcg_at_k, precision_at_k\n",
    "\n",
    "implicit_als = compute_als_decomposition(prev_matrix, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of   0%|<bar/>| 0/10 [03:30<?, ?it/s]>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s.yarkin/venv3_upd/lib/python3.6/site-packages/tqdm/std.py\", line 1150, in __del__\n",
      "    self.close()\n",
      "  File \"/home/s.yarkin/venv3_upd/lib/python3.6/site-packages/tqdm/notebook.py\", line 271, in close\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm' object has no attribute 'sp'\n",
      "Exception ignored in: <bound method tqdm.__del__ of   0%|<bar/>| 0/10 [01:31<?, ?it/s]>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s.yarkin/venv3_upd/lib/python3.6/site-packages/tqdm/std.py\", line 1150, in __del__\n",
      "    self.close()\n",
      "  File \"/home/s.yarkin/venv3_upd/lib/python3.6/site-packages/tqdm/notebook.py\", line 271, in close\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm' object has no attribute 'sp'\n"
     ]
    }
   ],
   "source": [
    "test_fold = pairs.loc[folds['test']].groupby(['customer_id', 'chain_id'])['target'].max().reset_index()\n",
    "test_matrix = create_item_user_matrix(test_fold, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9660x79885 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100324 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153421, 100)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_als.user_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11322, 100)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_als.item_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = previous_fold[['customer_id', 'chain_id', 'target']].copy()\n",
    "    \n",
    "unique_rows = list(np.unique(data['customer_id']).astype(np.int32))\n",
    "unique_cols = list(np.unique(data['chain_id']).astype(np.int32))\n",
    "\n",
    "row_map = dict(zip(unique_rows, range(len(unique_rows))))\n",
    "col_map = dict(zip(unique_cols, range(len(unique_cols))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for _, row in test.iterrows():\n",
    "    customer_idx = row_map.get(row.customer_id)\n",
    "    chain_idx = col_map.get(row.chain_id)\n",
    "    if not customer_idx or not chain_idx:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(cosine(implicit_als.user_factors[customer_idx].reshape(1,-1), \n",
    "                           implicit_als.item_factors[chain_idx].reshape(1,-1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1448623"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6681578, 142)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_upd",
   "language": "python",
   "name": "py3_upd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
